{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ffd62ef",
   "metadata": {},
   "source": [
    "# *Data load*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee42e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from pprint import pprint\n",
    "from pshmodule.utils import filemanager as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a7d5334",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9587ec355cfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n|\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"data load\")\n",
    "\n",
    "data = []\n",
    "with open(config.train_json, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line.rstrip('\\n|\\r')))\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2be131a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>오늘 반장 선거에서 내가 반장이 됐어 친구들이 날 믿어줘서 너무 고맙다 친구들에게 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>오늘 시험에 친구가 알려준 문제가 나왔어 결과가 좋아서 기쁘다 말로 하기 쑥스러운데...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>오늘 대학 합격 통지를 받았어 엄마가 너무 기뻐하는데 나는 눈물이 났어 사실 이렇게...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>오늘 졸업식인데 선생님께서 아프셔서 오지 못하셨어 그동안 감사하다는 말도 못했는데 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>나 올림피아드에서 금상 받았어 그동안 고생이 헛되지 않아서 너무 기뻐 근데 친구들이...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content label\n",
       "0  오늘 반장 선거에서 내가 반장이 됐어 친구들이 날 믿어줘서 너무 고맙다 친구들에게 ...     1\n",
       "1  오늘 시험에 친구가 알려준 문제가 나왔어 결과가 좋아서 기쁘다 말로 하기 쑥스러운데...     1\n",
       "2  오늘 대학 합격 통지를 받았어 엄마가 너무 기뻐하는데 나는 눈물이 났어 사실 이렇게...     1\n",
       "3  오늘 졸업식인데 선생님께서 아프셔서 오지 못하셨어 그동안 감사하다는 말도 못했는데 ...     1\n",
       "4  나 올림피아드에서 금상 받았어 그동안 고생이 헛되지 않아서 너무 기뻐 근데 친구들이...     1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9698d2e1",
   "metadata": {},
   "source": [
    "# *Definition*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee69d0f9",
   "metadata": {},
   "source": [
    "##### 사전 크기, 사용자 정의 토큰 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c6e6953",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f20c815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_defined_symbols = [\"<pad>\", \"<unk>\", \"<cls>\", \"<sep>\", \"<mask>\", \"<bos>\", \"<eos>\", \"<tsep>\", \"<name>\", \"<url>\"]\n",
    "user_defined_symbols += [\"<unk0>\", \"<unk1>\", \"<unk2>\", \"<unk3>\", \"<unk4>\", \"<unk5>\", \"<unk6>\", \"<unk7>\", \"<unk8>\", \"<unk9>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e82a7a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_token_num = 100\n",
    "unused_list = [f\"<unused{i}>\" for i in range(unused_token_num)]\n",
    "user_defined_symbols += unused_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cfcb101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>',\n",
      " '<unk>',\n",
      " '<cls>',\n",
      " '<sep>',\n",
      " '<mask>',\n",
      " '<bos>',\n",
      " '<eos>',\n",
      " '<tsep>',\n",
      " '<name>',\n",
      " '<url>',\n",
      " '<unk0>',\n",
      " '<unk1>',\n",
      " '<unk2>',\n",
      " '<unk3>',\n",
      " '<unk4>',\n",
      " '<unk5>',\n",
      " '<unk6>',\n",
      " '<unk7>',\n",
      " '<unk8>',\n",
      " '<unk9>',\n",
      " '<unused0>',\n",
      " '<unused1>',\n",
      " '<unused2>',\n",
      " '<unused3>',\n",
      " '<unused4>',\n",
      " '<unused5>',\n",
      " '<unused6>',\n",
      " '<unused7>',\n",
      " '<unused8>',\n",
      " '<unused9>',\n",
      " '<unused10>',\n",
      " '<unused11>',\n",
      " '<unused12>',\n",
      " '<unused13>',\n",
      " '<unused14>',\n",
      " '<unused15>',\n",
      " '<unused16>',\n",
      " '<unused17>',\n",
      " '<unused18>',\n",
      " '<unused19>',\n",
      " '<unused20>',\n",
      " '<unused21>',\n",
      " '<unused22>',\n",
      " '<unused23>',\n",
      " '<unused24>',\n",
      " '<unused25>',\n",
      " '<unused26>',\n",
      " '<unused27>',\n",
      " '<unused28>',\n",
      " '<unused29>',\n",
      " '<unused30>',\n",
      " '<unused31>',\n",
      " '<unused32>',\n",
      " '<unused33>',\n",
      " '<unused34>',\n",
      " '<unused35>',\n",
      " '<unused36>',\n",
      " '<unused37>',\n",
      " '<unused38>',\n",
      " '<unused39>',\n",
      " '<unused40>',\n",
      " '<unused41>',\n",
      " '<unused42>',\n",
      " '<unused43>',\n",
      " '<unused44>',\n",
      " '<unused45>',\n",
      " '<unused46>',\n",
      " '<unused47>',\n",
      " '<unused48>',\n",
      " '<unused49>',\n",
      " '<unused50>',\n",
      " '<unused51>',\n",
      " '<unused52>',\n",
      " '<unused53>',\n",
      " '<unused54>',\n",
      " '<unused55>',\n",
      " '<unused56>',\n",
      " '<unused57>',\n",
      " '<unused58>',\n",
      " '<unused59>',\n",
      " '<unused60>',\n",
      " '<unused61>',\n",
      " '<unused62>',\n",
      " '<unused63>',\n",
      " '<unused64>',\n",
      " '<unused65>',\n",
      " '<unused66>',\n",
      " '<unused67>',\n",
      " '<unused68>',\n",
      " '<unused69>',\n",
      " '<unused70>',\n",
      " '<unused71>',\n",
      " '<unused72>',\n",
      " '<unused73>',\n",
      " '<unused74>',\n",
      " '<unused75>',\n",
      " '<unused76>',\n",
      " '<unused77>',\n",
      " '<unused78>',\n",
      " '<unused79>',\n",
      " '<unused80>',\n",
      " '<unused81>',\n",
      " '<unused82>',\n",
      " '<unused83>',\n",
      " '<unused84>',\n",
      " '<unused85>',\n",
      " '<unused86>',\n",
      " '<unused87>',\n",
      " '<unused88>',\n",
      " '<unused89>',\n",
      " '<unused90>',\n",
      " '<unused91>',\n",
      " '<unused92>',\n",
      " '<unused93>',\n",
      " '<unused94>',\n",
      " '<unused95>',\n",
      " '<unused96>',\n",
      " '<unused97>',\n",
      " '<unused98>',\n",
      " '<unused99>']\n"
     ]
    }
   ],
   "source": [
    "pprint(user_defined_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c88331",
   "metadata": {},
   "source": [
    "# *Tokenizer Train*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5914cd84",
   "metadata": {},
   "source": [
    "##### Huggingface BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea96fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, trainers\n",
    "from transformers import GPT2TokenizerFast, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "729be72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer : <tokenizers.Tokenizer object at 0x88b0c90>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(models.BPE())\n",
    "print(f\"tokenizer : {tokenizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b83a82b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.NFKC()\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.Metaspace()])\n",
    "tokenizer.decoders = decoders.Metaspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "909cb34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=vocab_size, \n",
    "    show_progress=True,\n",
    "    special_tokens=user_defined_symbols,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "140c295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen():\n",
    "    for row in df.content:\n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cd0ab7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(gen(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97f1ac6d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "rm: cannot remove '../../tokenizer/temps': No such file or directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../../tokenizer/temp/vocab.json', '../../tokenizer/temp/merges.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model.save('../../tokenizer/temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143fd515",
   "metadata": {},
   "source": [
    "##### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb071515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2107, 1754, 10967, 4617, 12549, 4439, 1885, 1876]\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(\"본 고안은 이러한 특성을 이용해 사용한다.\")\n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d2867ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁본 ▁고 안은 ▁이러한 ▁특성을 ▁이용해 ▁사용 한다.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92248c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁본▁고안은▁이러한▁특성을▁이용해▁사용한다.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decoder = decoders.BPEDecoder(suffix='_')\n",
    "tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588db695",
   "metadata": {},
   "source": [
    "# *Save according to form*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e23e899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_for_load = GPT2TokenizerFast.from_pretrained(\"../../tokenizer/temp\")  # 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222b6118",
   "metadata": {},
   "source": [
    "##### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "135401bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_for_load.pad_token = \"<pad>\"\n",
    "tokenizer_for_load.unk_token = \"<unk>\"\n",
    "tokenizer_for_load.cls_token = \"<cls>\"\n",
    "tokenizer_for_load.sep_token = \"<sep>\"\n",
    "tokenizer_for_load.mask_token = \"<mask>\"\n",
    "tokenizer_for_load.bos_token = \"<bos>\"\n",
    "tokenizer_for_load.eos_token = \"<eos>\"\n",
    "\n",
    "special_tokens_dict = {'additional_special_tokens': user_defined_symbols}\n",
    "tokenizer_for_load.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0245d285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../tokenizer/tokenizer_config.json',\n",
       " '../../tokenizer/special_tokens_map.json',\n",
       " '../../tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_for_load.save_pretrained(\"../../tokenizer\", legacy_format=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048d9028",
   "metadata": {},
   "source": [
    "in `tokenizer.json`  \n",
    "```json\n",
    "    \"normalizer\": {\n",
    "        \"type\": \"Sequence\",\n",
    "        \"normalizers\": [\n",
    "            {\n",
    "                \"type\": \"NFKC\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"BertNormalizer\",\n",
    "                \"clean_text\": false,\n",
    "                \"handle_chinese_chars\": false,\n",
    "                \"strip_accents\": false,\n",
    "                \"lowercase\": false\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"pre_tokenizer\": {\n",
    "        \"type\": \"Sequence\",\n",
    "        \"pretokenizers\": [\n",
    "            {\n",
    "                \"type\": \"Metaspace\",\n",
    "                \"replacement\": \"▁\",\n",
    "                \"add_prefix_space\": true\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"post_processor\": null,\n",
    "    \"decoder\": {\n",
    "        \"type\": \"Metaspace\",\n",
    "        \"replacement\": \"▁\",\n",
    "        \"add_prefix_space\": true\n",
    "    },\n",
    "```\n",
    "\n",
    "in `tokenizer_config.json`  \n",
    "```json\n",
    ",\n",
    "    \"model_type\": \"gpt2\"\n",
    "```\n",
    "\n",
    "rename `tokenizer_config.json` => `config.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7313e6ae",
   "metadata": {},
   "source": [
    "# *Test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72c60c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = AutoTokenizer.from_pretrained(\"../../tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e8b90f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2107, 1754, 10967, 4617, 12549, 4439, 1885, 1876], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "본 고안은 이러한 특성을 이용해 사용한다.\n"
     ]
    }
   ],
   "source": [
    "e = t(\"본 고안은 이러한 특성을 이용해 사용한다.\")\n",
    "print(e)\n",
    "print(t.decode(e['input_ids']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
