{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ffd62ef",
   "metadata": {},
   "source": [
    "# *Data load*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee42e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import config\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from pshmodule.utils import filemanager as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a7d5334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load\n"
     ]
    }
   ],
   "source": [
    "print(\"data load\")\n",
    "\n",
    "data = []\n",
    "with open(config.train_json, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line.rstrip('\\n|\\r')))\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2be131a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>room_no</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>이제 위드 코로나 하면 회식 시작하겠구만</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11월달부터 위드 코로나 하려나 보던데요</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그러게 슬슬 준비하고 있네</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>회식 좋아하는 사람들에게 희소식이겠네요</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>그러게 신입 사원들은 이제 큰일이네</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  content  room_no speaker\n",
       "0  이제 위드 코로나 하면 회식 시작하겠구만        0       1\n",
       "1  11월달부터 위드 코로나 하려나 보던데요        0       2\n",
       "2          그러게 슬슬 준비하고 있네        0       1\n",
       "3   회식 좋아하는 사람들에게 희소식이겠네요        0       2\n",
       "4     그러게 신입 사원들은 이제 큰일이네        0       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9698d2e1",
   "metadata": {},
   "source": [
    "# *Definition*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee69d0f9",
   "metadata": {},
   "source": [
    "##### 사전 크기, 사용자 정의 토큰 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c6e6953",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f20c815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_defined_symbols = [\"<pad>\", \"<unk>\", \"<cls>\", \"<sep>\", \"<mask>\", \"<bos>\", \"<eos>\", \"<tsep>\", \"<name>\", \"<url>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e82a7a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_token_num = 100\n",
    "unused_list = [f\"<unused{i}>\" for i in range(unused_token_num)]\n",
    "user_defined_symbols += unused_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cfcb101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>',\n",
      " '<unk>',\n",
      " '<cls>',\n",
      " '<sep>',\n",
      " '<mask>',\n",
      " '<bos>',\n",
      " '<eos>',\n",
      " '<tsep>',\n",
      " '<name>',\n",
      " '<url>',\n",
      " '<unused0>',\n",
      " '<unused1>',\n",
      " '<unused2>',\n",
      " '<unused3>',\n",
      " '<unused4>',\n",
      " '<unused5>',\n",
      " '<unused6>',\n",
      " '<unused7>',\n",
      " '<unused8>',\n",
      " '<unused9>',\n",
      " '<unused10>',\n",
      " '<unused11>',\n",
      " '<unused12>',\n",
      " '<unused13>',\n",
      " '<unused14>',\n",
      " '<unused15>',\n",
      " '<unused16>',\n",
      " '<unused17>',\n",
      " '<unused18>',\n",
      " '<unused19>',\n",
      " '<unused20>',\n",
      " '<unused21>',\n",
      " '<unused22>',\n",
      " '<unused23>',\n",
      " '<unused24>',\n",
      " '<unused25>',\n",
      " '<unused26>',\n",
      " '<unused27>',\n",
      " '<unused28>',\n",
      " '<unused29>',\n",
      " '<unused30>',\n",
      " '<unused31>',\n",
      " '<unused32>',\n",
      " '<unused33>',\n",
      " '<unused34>',\n",
      " '<unused35>',\n",
      " '<unused36>',\n",
      " '<unused37>',\n",
      " '<unused38>',\n",
      " '<unused39>',\n",
      " '<unused40>',\n",
      " '<unused41>',\n",
      " '<unused42>',\n",
      " '<unused43>',\n",
      " '<unused44>',\n",
      " '<unused45>',\n",
      " '<unused46>',\n",
      " '<unused47>',\n",
      " '<unused48>',\n",
      " '<unused49>',\n",
      " '<unused50>',\n",
      " '<unused51>',\n",
      " '<unused52>',\n",
      " '<unused53>',\n",
      " '<unused54>',\n",
      " '<unused55>',\n",
      " '<unused56>',\n",
      " '<unused57>',\n",
      " '<unused58>',\n",
      " '<unused59>',\n",
      " '<unused60>',\n",
      " '<unused61>',\n",
      " '<unused62>',\n",
      " '<unused63>',\n",
      " '<unused64>',\n",
      " '<unused65>',\n",
      " '<unused66>',\n",
      " '<unused67>',\n",
      " '<unused68>',\n",
      " '<unused69>',\n",
      " '<unused70>',\n",
      " '<unused71>',\n",
      " '<unused72>',\n",
      " '<unused73>',\n",
      " '<unused74>',\n",
      " '<unused75>',\n",
      " '<unused76>',\n",
      " '<unused77>',\n",
      " '<unused78>',\n",
      " '<unused79>',\n",
      " '<unused80>',\n",
      " '<unused81>',\n",
      " '<unused82>',\n",
      " '<unused83>',\n",
      " '<unused84>',\n",
      " '<unused85>',\n",
      " '<unused86>',\n",
      " '<unused87>',\n",
      " '<unused88>',\n",
      " '<unused89>',\n",
      " '<unused90>',\n",
      " '<unused91>',\n",
      " '<unused92>',\n",
      " '<unused93>',\n",
      " '<unused94>',\n",
      " '<unused95>',\n",
      " '<unused96>',\n",
      " '<unused97>',\n",
      " '<unused98>',\n",
      " '<unused99>']\n"
     ]
    }
   ],
   "source": [
    "pprint(user_defined_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c88331",
   "metadata": {},
   "source": [
    "# *Tokenizer Train*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5914cd84",
   "metadata": {},
   "source": [
    "##### Huggingface BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea96fa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, trainers\n",
    "from transformers import GPT2TokenizerFast, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "729be72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer : <tokenizers.Tokenizer object at 0x55bc44353040>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(models.BPE())\n",
    "print(f\"tokenizer : {tokenizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b83a82b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.NFKC()\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.Metaspace()])\n",
    "tokenizer.decoders = decoders.Metaspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "909cb34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=vocab_size, \n",
    "    show_progress=True,\n",
    "    special_tokens=user_defined_symbols,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "140c295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen():\n",
    "    for row in df.content:\n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cd0ab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(gen(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97f1ac6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/shpark-datashare/generation_model/GPTNeoX/outputs/tokenizer/temp/vocab.json',\n",
       " '/home/jovyan/shpark-datashare/generation_model/GPTNeoX/outputs/tokenizer/temp/merges.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model.save(config.tokenizer_path+\"temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143fd515",
   "metadata": {},
   "source": [
    "##### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb071515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3099, 2887, 19238, 4120, 2643, 3344, 7225, 8284, 4001, 20697]\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(\"본 고안은 이러한 특성을 이용해 사용한다.\")\n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d2867ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁본 ▁고 안은 ▁이러 한 ▁특 성을 ▁이용해 ▁사용 한다.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92248c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁본▁고안은▁이러한▁특성을▁이용해▁사용한다.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decoder = decoders.BPEDecoder(suffix='_')\n",
    "tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588db695",
   "metadata": {},
   "source": [
    "# *Save according to form*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e23e899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_for_load = GPT2TokenizerFast.from_pretrained(config.tokenizer_path+\"temp\")  # 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222b6118",
   "metadata": {},
   "source": [
    "##### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "135401bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_for_load.pad_token = \"<pad>\"\n",
    "tokenizer_for_load.unk_token = \"<unk>\"\n",
    "tokenizer_for_load.cls_token = \"<cls>\"\n",
    "tokenizer_for_load.sep_token = \"<sep>\"\n",
    "tokenizer_for_load.mask_token = \"<mask>\"\n",
    "tokenizer_for_load.bos_token = \"<bos>\"\n",
    "tokenizer_for_load.eos_token = \"<eos>\"\n",
    "\n",
    "special_tokens_dict = {'additional_special_tokens': user_defined_symbols}\n",
    "tokenizer_for_load.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0245d285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/jovyan/shpark-datashare/generation_model/GPTNeoX/outputs/tokenizer/tokenizer_config.json',\n",
       " '/home/jovyan/shpark-datashare/generation_model/GPTNeoX/outputs/tokenizer/special_tokens_map.json',\n",
       " '/home/jovyan/shpark-datashare/generation_model/GPTNeoX/outputs/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_for_load.save_pretrained(config.tokenizer_path, legacy_format=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048d9028",
   "metadata": {},
   "source": [
    "in `tokenizer.json`  \n",
    "```json\n",
    "    \"normalizer\": {\n",
    "        \"type\": \"Sequence\",\n",
    "        \"normalizers\": [\n",
    "            {\n",
    "                \"type\": \"NFKC\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"BertNormalizer\",\n",
    "                \"clean_text\": false,\n",
    "                \"handle_chinese_chars\": false,\n",
    "                \"strip_accents\": false,\n",
    "                \"lowercase\": false\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"pre_tokenizer\": {\n",
    "        \"type\": \"Sequence\",\n",
    "        \"pretokenizers\": [\n",
    "            {\n",
    "                \"type\": \"Metaspace\",\n",
    "                \"replacement\": \"▁\",\n",
    "                \"add_prefix_space\": true\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"post_processor\": null,\n",
    "    \"decoder\": {\n",
    "        \"type\": \"Metaspace\",\n",
    "        \"replacement\": \"▁\",\n",
    "        \"add_prefix_space\": true\n",
    "    },\n",
    "```\n",
    "\n",
    "in `tokenizer_config.json`  \n",
    "```json\n",
    ",\n",
    "    \"model_type\": \"gpt2\"\n",
    "```\n",
    "\n",
    "rename `tokenizer_config.json` => `config.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7313e6ae",
   "metadata": {},
   "source": [
    "# *Test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72c60c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = AutoTokenizer.from_pretrained(config.tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e8b90f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [3099, 2887, 19238, 4120, 2643, 3344, 7225, 8284, 4001, 20697], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "본 고안은 이러한 특성을 이용해 사용한다.\n"
     ]
    }
   ],
   "source": [
    "e = t(\"본 고안은 이러한 특성을 이용해 사용한다.\")\n",
    "print(e)\n",
    "print(t.decode(e['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b7348-4059-43f1-b22c-a177be49418b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
